---
layout: post
title:  "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning"
date:   2020-11-18 00:00:00 +00:00
image: images/nsl.gif
categories: research
author: "Ryan Julian"
authors: "<strong>Ryan Julian</strong>, Benjamin Swanson, Gaurav S. Sukhatme, Sergey Levine, Chelsea Finn, Karol Hausman"
venue: "Conference on Robot Learning"
arxiv: https://arxiv.org/abs/2004.10190
slides: /pdfs/nsl_slides.pdf
site: /never-stop-learning
video: https://www.youtube.com/watch?v=pPDVewcSpdc
two_minute_papers: https://www.youtube.com/watch?v=9gX24m3kcjA
talk: https://youtu.be/HLJG_lPT4y0
venture_beat: https://venturebeat.com/2020/04/22/googles-ai-teaches-robots-to-grasp-occluded-objects-and-adapt-to-new-situations/
usc_news: https://viterbischool.usc.edu/news/2020/12/conference-on-robotic-learning-teaching-robots-to-cook-navigate-and-learn-from-mistakes/
---

We formulate a fine-tuning procedure for off-policy reinforcement learning which is simple, fast, effective, and sample-efficient. We then show that it can be used to adapt robotic manipulation policies to novel environments, lighting conditions, objects, robot wear-and-tear, etc. in a continual learning setting.
